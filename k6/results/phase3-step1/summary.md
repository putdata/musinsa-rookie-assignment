# Phase 3 Step 1 부하테스트 결과

**측정 일시:** 2026-02-21
**실행 방법:** java -jar (직접 JAR 실행)
**k6 시나리오:** queue-enrollment-rush.js (Burst + Sustained 패턴)
**테스트 대상:** phase3-step1 (Redis Queue 기반 수강신청)
**측정 횟수:** 각 조건 2회 측정 후 평균

---

## 1. VU 확장성 테스트 (톰캣 1000 스레드 고정)

| 지표 | 1000 VU | 2000 VU | 3000 VU |
|------|:---:|:---:|:---:|
| **에러율** | 0% | 0% | 0% |
| **수강 성공** | ~3,952 | ~3,957 | ~3,957 |
| **Submit p95** | 3.5ms | 7.7ms | 62ms |
| **Submit p95 (Burst)** | 4.3ms | 11ms | 72.8ms |
| **Submit p95 (Sustained)** | 2.5ms | 2.6ms | 2.8ms |
| **Wait p95** | 10.2s | 21s | 31.5s |
| **iterations/s** | 115 | 120 | 122 |
| **req/s** | 976 | 1,443 | 1,828 |

### 분석

- **에러율 0%**: 3000 VU에서도 서버 에러 없이 전량 처리. 큐 아키텍처의 안정성 검증됨.
- **수강 성공 ~3,957건 일정**: 정원 제한이 정확히 동작. 동시성 제어 완벽.
- **Submit p95**: 1000→2000 VU는 선형 증가(3.5→7.7ms), 3000 VU에서 급증(62ms). 톰캣 스레드 포화로 커넥션 큐잉 발생.
- **Sustained 구간**: 3000 VU에서도 p95 2.8ms로 안정. 큐가 순간 폭주를 완충하는 효과.
- **Wait 시간**: VU에 정비례 증가 (10s → 21s → 31.5s). 큐 FIFO 특성상 대기열 길이에 비례.
- **iterations/s ~120으로 수렴**: VU를 3배 늘려도 실처리량은 거의 동일. **초당 ~120건이 시스템 처리 한계**. req/s 증가는 폴링 요청 증가에 의한 것.

---

## 2. 톰캣 스레드풀 비교 (3000 VU 고정)

| 지표 | 500t | 1000t | 2000t |
|------|:---:|:---:|:---:|
| **에러율** | 0% | 0% | 0% |
| **수강 성공** | 3,957 | 3,957 | 3,954 |
| **Submit p95** | 173ms | 62ms | 60.5ms |
| **Submit p95 (Burst)** | 187.6ms | 72.8ms | 66.6ms |
| **Submit p95 (Sustained)** | 2.8ms | 2.8ms | 2.5ms |
| **Wait p95** | 31.2s | 31.5s | 54.1s |
| **iterations/s** | 123.2 | 122.1 | 67.6 |
| **req/s** | 1,821 | 1,828 | 1,884 |

### 분석

#### 1000t가 최적점 (Sweet Spot)

- **500t**: Submit은 느리지만(173ms) 실처리량은 1000t와 동일(123 iter/s)
- **1000t**: Submit 빠르고(62ms) 처리량도 최대(122 iter/s) — 균형 잡힌 최적
- **2000t**: Submit은 1000t와 비슷하지만(60.5ms) 처리량이 **45% 하락**(67.6 iter/s)

#### 500t vs 1000t: Submit만 다르고 처리량은 동일한 이유

3000 VU가 동시에 POST 요청을 보낼 때:

```
3000 VU → [톰캣 Accept Queue] → [톰캣 스레드풀] → Redis ZADD → 202 응답
```

- **1000t**: 1000개 스레드가 동시 처리, 나머지 2000은 짧은 대기 후 2~3 라운드로 소화 → p95 62ms
- **500t**: 500개만 동시 처리, 2500이 대기 → 5~6 라운드 필요 → p95 173ms (약 3배)

Submit은 "큐에 넣는 입구"일 뿐이고, 실처리 속도는 **큐 워커**가 결정한다.
큐 워커는 톰캣 스레드풀과 독립적으로 동작하므로, 입구가 좁든 넓든 처리량은 동일하다.

> 비유: 매표소 창구 수(Submit) vs 놀이기구 탑승 속도(Wait). 창구가 적으면 표 사는 데 오래 걸리지만, 탑승 속도는 동일.

#### 2000t가 오히려 느린 이유: 스레드 과다의 역효과

2000개 스레드가 동시에 활성화되면:

1. **폴링 GET 요청** 2000개가 동시에 Redis를 조회 → Redis 커넥션/CPU 경합
2. 큐 워커가 Redis에 결과를 쓰려는데, 폴링 스레드들과 **리소스 경합**
3. 큐 워커의 처리 속도 자체가 저하 → Wait 54s, iterations/s 67로 하락
4. 컨텍스트 스위칭 오버헤드 증가

---

## 3. 종합 결론

### 최적 구성

| 항목 | 권장 값 | 근거 |
|------|---------|------|
| 톰캣 스레드 | **1000** | Submit 속도와 처리량 모두 최적 |
| 운영 가능 VU | **~2000** | Submit p95 < 10ms, Wait < 21s |
| 시스템 처리 한계 | **~120 iter/s** | VU 증가와 무관하게 수렴 |

### 핵심 발견

1. **큐 아키텍처의 안정성**: 모든 조건에서 에러율 0%, 정원 초과 0건
2. **스레드는 많다고 좋지 않다**: 2000t는 1000t 대비 처리량 45% 하락
3. **병목은 큐 워커**: 톰캣 스레드가 아닌 큐 처리 속도가 실질적 처리량을 결정
4. **Sustained 구간 안정**: 어떤 조건에서든 Sustained Submit p95은 2.5~3ms로 일정
